/// Don't delete any field from the schema once released.
/// Asign a id for each filed to keep compatibility easily.

// The below definitions of Record and RecordBatch are inspired by Kafka's protocols
table RecordBatchMeta {
    /// The stream name of this record batch.
    stream_name: string (id: 0);
    /// The record format version of this record batch.
    magic: byte (id: 1);
    /// The attributes of this record batch. Each bit is used to indicate a specific attribute.
    attributes: byte (id: 2);
    /// The base offset of the batch record, also is the logical offset of the first record.
    base_offset: int64 (id: 3);
    /// The delta value between the last offset and the base offset. 
    last_offset_delta: int32 (id: 4);
    /// The create timestap of the first record in this batch.
    first_timestamp: int64 (id: 5);
    /// The max timestamp among all records contained in this batch.
    max_timestamp: int64 (id: 6);
}

table Property {
    key: string (id: 0);
    value: string (id: 1);
}

/// Enum values should only ever be added. Never remove or renumber them.
enum SystemKeys : byte {
    /// The tag marks the record, is used to filter records in server side.
    Tag = 0,
    /// The list of index keys for records.
    Keys,
    /// The unique id of the record. It's not neccessary to be unique in the whole cluster.
    RecordId,
}

table Header {
    key: SystemKeys (id: 0);
    value: string (id: 1);
}

table RecordMeta {
    offset_delta: int32 (id: 0);
    timestamp_delta: int32 (id: 1);
    headers: [Header] (id: 2);
    properties: [Property] (id: 3);
}